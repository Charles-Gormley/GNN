{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 01:30:05.139131: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-24 01:30:05.139173: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-24 01:30:05.140774: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-24 01:30:05.277451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spektral\n",
    "\n",
    "from spektral.layers import GraphConv\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 118.65608978271484\n",
      "Epoch 10, Loss: 68.46800231933594\n",
      "Epoch 20, Loss: 5.2134575843811035\n",
      "Epoch 30, Loss: 4.327686786651611\n",
      "Epoch 40, Loss: 3.3659143447875977\n",
      "Epoch 50, Loss: 11.481392860412598\n",
      "Epoch 60, Loss: 61.27983093261719\n",
      "Epoch 70, Loss: 7.159087657928467\n",
      "Epoch 80, Loss: 5.361532211303711\n",
      "Epoch 90, Loss: 24.83232307434082\n",
      "Epoch 100, Loss: 6.735262393951416\n",
      "Epoch 110, Loss: 14.21876049041748\n",
      "Epoch 120, Loss: 12.98305606842041\n",
      "Epoch 130, Loss: 6.100507736206055\n",
      "Epoch 140, Loss: 10.257633209228516\n",
      "Epoch 150, Loss: 28.92128562927246\n",
      "Epoch 160, Loss: 33.12507247924805\n",
      "Epoch 170, Loss: 3.5632517337799072\n",
      "Epoch 180, Loss: 7.786409854888916\n",
      "Epoch 190, Loss: 0.00377857219427824\n",
      "tensor([[ 9.7425],\n",
      "        [11.5521],\n",
      "        [ 8.7654]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Example node features (each row corresponds to a node's feature)\n",
    "# Replace these with your actual data\n",
    "node_features = torch.tensor([[2.5, 3.0], [5.5, 4.2], [3.1, 3.7]], dtype=torch.float)\n",
    "\n",
    "# Example edge index (defining the graph structure)\n",
    "edge_index = torch.tensor([[0, 1, 2, 0], # This is going to be the graph direction. edge 0 --> \n",
    "                           [1, 0, 1, 2]], \n",
    "                           dtype=torch.long)\n",
    "\n",
    "# Example static edge weights\n",
    "# Replace these with your actual data or a method to compute them\n",
    "edge_weights = torch.tensor([0.5, 0.7, 0.9, 0.4], dtype=torch.float) # Weights of each edge connection.\n",
    "\n",
    "# Creating a graph with edge weights\n",
    "graph = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weights)\n",
    "\n",
    "# GCN Model Definition\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(graph.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 1)  # Output size is 1\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Dummy target values (prices) for demonstration\n",
    "# Replace these with your actual target values\n",
    "target_values = torch.tensor([10.0, 12.0, 11.5], dtype=torch.float)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph)\n",
    "    loss = criterion(out.view(-1), target_values)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predicted_values = model(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7563823461532593\n",
      "Epoch 10, Loss: 0.6653655767440796\n",
      "Epoch 20, Loss: 0.6533530950546265\n",
      "Epoch 30, Loss: 0.5221158266067505\n",
      "Epoch 40, Loss: 0.4485265612602234\n",
      "Epoch 50, Loss: 0.49332723021507263\n",
      "Epoch 60, Loss: 0.38029158115386963\n",
      "Epoch 70, Loss: 0.5505840182304382\n",
      "Epoch 80, Loss: 0.3836038112640381\n",
      "Epoch 90, Loss: 0.5117744207382202\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class GNNTransformer(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, num_heads=4, num_layers=2, dim_feedforward=128):\n",
    "        super(GNNTransformer, self).__init__()\n",
    "        \n",
    "        # GNN part\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "\n",
    "        # Transformer part\n",
    "        self.pos_encoder = PositionalEncoding(16, dropout=0.1)\n",
    "        transformer_layer = TransformerEncoderLayer(d_model=16, nhead=num_heads, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # GNN part\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Reshape for Transformer\n",
    "        x = x.unsqueeze(1)  # Add a fake sequence dimension (suitable for Transformer)\n",
    "\n",
    "        # Transformer part\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = x.squeeze(1)  # Remove the fake sequence dimension\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "num_node_features = 3  # Replace with your actual number of node features\n",
    "num_classes = 1  # Replace with your actual number of classes (for classification) or 1 (for regression)\n",
    "\n",
    "model = GNNTransformer(num_node_features, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "node_features = torch.rand(10, num_node_features)  # 10 nodes, each with 'num_node_features' features\n",
    "edge_index = torch.randint(0, 10, (2, 20))  # 20 edges randomly connected\n",
    "data = torch_geometric.data.Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# Generate synthetic labels for demonstration (random integers for classification)\n",
    "labels = torch.randint(0, num_classes, (10,))  # Assuming 10 nodes\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out, labels)  # Use cross_entropy for classification\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch GNN-Transformer Model Explanation\n",
    "\n",
    "## Importing Libraries\n",
    "- `torch`: The main PyTorch library, used for tensor operations and neural network components.\n",
    "- `torch.nn`: Submodule containing classes to help create neural network layers.\n",
    "- `torch.optim`: Contains optimizers like Adam, used for updating network weights.\n",
    "- `torch.nn.functional`: Functional API that includes methods like activation functions.\n",
    "- `torch_geometric.nn`: PyTorch Geometric's neural network module, specifically for graph neural networks.\n",
    "- `math`: Standard Python library for mathematical operations.\n",
    "\n",
    "## Positional Encoding Class\n",
    "- Adds positional encodings to inputs, critical in Transformer models. Positional encodings give a sense of order or position to the model.\n",
    "- `__init__`: Initializes the positional encoding with a specific dropout rate and precomputes positional encodings.\n",
    "- `forward`: Adds positional encodings to the input tensor `x` and applies dropout.\n",
    "\n",
    "## GNNTransformer Class\n",
    "- A custom neural network module combining a Graph Convolutional Network (GCN) with a Transformer.\n",
    "- `__init__`: Initializes GCN layers, positional encoding, transformer encoder, and final output linear layer.\n",
    "- `forward`: Defines the forward pass. Processes input data using GCN layers, reshapes for the Transformer, applies Transformer encoder, and outputs through a linear layer.\n",
    "\n",
    "## Model Initialization\n",
    "- `num_node_features` and `num_classes`: Number of features per node and output classes.\n",
    "- Creates an instance of `GNNTransformer` and sets up an Adam optimizer.\n",
    "\n",
    "## Data Preparation\n",
    "- Synthetic node features and edge indices created to represent a graph.\n",
    "- `labels`: Random synthetic labels for each node, used as training targets.\n",
    "\n",
    "## Training Loop\n",
    "- Trains the model for a number of epochs.\n",
    "- In each epoch, zeroes optimizer gradients, runs forward pass, calculates loss using cross-entropy, and backpropagates to update model parameters.\n",
    "- Prints the loss every 10 epochs to monitor progress.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GNN-Transformer Model for Time Series Prediction\n",
    "\n",
    "## 1. Determining the Lag Length\n",
    "- Determine the optimal lag length for the time series data.\n",
    "- The lag length can be based on domain knowledge, experimentation, or time series cross-validation.\n",
    "\n",
    "## 2. Preparing the Training Data\n",
    "- Structure the training data so that each prediction uses the previous `N` days (lag length) of data.\n",
    "- For each node, create input sequences of node features over the lag window.\n",
    "- The target for each sequence is the market price at the next time point.\n",
    "\n",
    "## 3. Sequential Training Approach\n",
    "- Train the model using a rolling window approach, where it predicts the next point based on the previous lagged sequence.\n",
    "- For example, use time points 1 to 10 to predict point 11, then use points 2 to 11 to predict point 12, and so on.\n",
    "\n",
    "## 4. Handling Sequential Data in Transformers\n",
    "- The Transformer processes these sequences with positional encodings that reflect the sequence's order.\n",
    "- It learns patterns within these sequences, aided by the graph context from the GNN.\n",
    "\n",
    "## 5. Training Batches and Randomization\n",
    "- Randomize the order of training samples for better generalization.\n",
    "- Maintain the correct temporal order within each individual sequence.\n",
    "\n",
    "## 6. Model Evaluation and Validation\n",
    "- Use a validation set to evaluate performance on unseen data.\n",
    "- Consider time series cross-validation for effective hyperparameter tuning.\n",
    "\n",
    "## 7. Additional Considerations\n",
    "- Be aware of the risks of overfitting and apply regularization techniques as needed.\n",
    "- Preprocess financial time series data carefully and consider including additional relevant features.\n",
    "\n",
    "This training approach leverages the temporal dynamics of each entity and the relational dynamics within the graph, suitable for financial time series data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using Positional Encodings for Sequential Time Series Data\n",
    "\n",
    "## Purpose of Positional Encodings\n",
    "- Transformers lack an inherent mechanism to recognize the order of inputs in a sequence. Positional encodings provide information about the sequence order to the Transformer.\n",
    "\n",
    "## Generating Positional Encodings\n",
    "- Positional encodings are generated using sinusoidal functions of different frequencies for each position `pos` in the sequence and dimension `i` in the embedding.\n",
    "- The formulas for positional encoding `pos_enc` are:\n",
    "  - `pos_enc(pos, 2i) = sin(pos / 10000^(2i/d_model))`\n",
    "  - `pos_enc(pos, 2i+1) = cos(pos / 10000^(2i/d_model))`\n",
    "- `d_model` is the dimension of the embedding, and `pos` is the position in the sequence.\n",
    "\n",
    "## Applying Positional Encodings\n",
    "- The positional encoding vector is added to the input embedding vector for each time step in the sequence.\n",
    "- The `pos` value corresponds to the temporal order in the sequence.\n",
    "- For different time steps in a sequence, `pos` is calculated accordingly (e.g., 0 for the first day, 1 for the second day, etc.).\n",
    "\n",
    "## Handling Different Sequence Lengths\n",
    "- Generate positional encodings up to the maximum sequence length to accommodate sequences of different lengths.\n",
    "- Apply masking in the Transformer to ignore padded positions in sequences.\n",
    "\n",
    "## Training and Prediction\n",
    "- Ensure that each input sequence to the Transformer has the correct positional encodings added during both training and prediction.\n",
    "- This approach allows the model to understand the temporal relationship between different time steps in the sequence.\n",
    "\n",
    "Positional encodings are crucial for the Transformer to understand the order of events in time series data, making more informed predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
